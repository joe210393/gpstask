#!/usr/bin/env python3
"""
è‡ªå‹•è£œè©è…³æœ¬ - æƒæå…¨åº«æ‰¾å‡ºæœªæ˜ å°„çš„ key_featuresï¼Œè‡ªå‹•å»ºè­°è£œé½Šè©å½™è¡¨

åŠŸèƒ½ï¼š
1. æƒæå…¨åº«çš„ key_features
2. æ‰¾å‡ºæœªè¢« trait_vocab.json æ˜ å°„çš„ token
3. ä½¿ç”¨è¦å‰‡å»ºè­°åˆ†é¡
4. ç”¢å‡º patch æª”å’Œå ±è¡¨

è¼¸å‡ºï¼š
- unmapped_top.csvï¼šæœ€å¸¸è¦‹ã€Œæœªæ˜ å°„ tokenã€Top N
- suggested_patch.jsonï¼šè‡ªå‹•å»ºè­°è¦æ–°å¢åˆ° vocab çš„åŒç¾©è©
- mapping_report.csvï¼šæ¯å€‹ token çš„æ˜ å°„çµæœ
"""

import json
import re
import collections
import csv
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# è·¯å¾‘è¨­å®š
SCRIPT_DIR = Path(__file__).parent
JSONL_PATH = SCRIPT_DIR.parent / "data" / "plants-forest-gov-tw-enhanced.jsonl"
if not JSONL_PATH.exists():
    JSONL_PATH = SCRIPT_DIR.parent / "data" / "plants-forest-gov-tw.jsonl"

OUT_UNMAPPED_CSV = SCRIPT_DIR / "unmapped_top.csv"
OUT_PATCH_JSON = SCRIPT_DIR / "suggested_patch.json"
OUT_REPORT_CSV = SCRIPT_DIR / "mapping_report.csv"

# è¼‰å…¥è©å½™è¡¨
VOCAB_PATH = SCRIPT_DIR / "trait_vocab.json"
with VOCAB_PATH.open("r", encoding="utf-8") as f:
    VOCAB = json.load(f)

# å»ºç«‹åå‘æ˜ å°„è¡¨ï¼šzh token -> (trait, canon)
REVERSE_MAP: Dict[str, Tuple[str, str]] = {}
for trait, values in VOCAB.items():
    for canon, data in values.items():
        for zh in data.get("zh", []):
            REVERSE_MAP[zh] = (trait, canon)


def normalize_token(t: str) -> str:
    """æ­£è¦åŒ– tokenï¼ˆå»é™¤æ¨™é»ã€ç©ºç™½ï¼‰"""
    if not t:
        return ""
    t = re.sub(r"[ï¼ˆï¼‰()ã€,ï¼Œ;ï¼›ã€‚\.]+", "", t)
    t = re.sub(r"\s+", "", t)
    return t.strip()


def heuristic_map(tok: str) -> Optional[Tuple[str, str, str]]:
    """
    å•Ÿç™¼å¼æ˜ å°„ï¼šdirect -> strip -> contains
    
    Returns:
        (trait, canonical, method) æˆ– None
    """
    tok = normalize_token(tok)
    if not tok:
        return None
    
    # 1. Direct match
    if tok in REVERSE_MAP:
        trait, canon = REVERSE_MAP[tok]
        return trait, canon, "direct"
    
    # 2. Strip common suffixes
    stripped = re.sub(r"(è‘‰åº|è‘‰ç·£|è‘‰é‚Š|è‘‰ç‰‡|è‘‰|èŠ±åº|æœå¯¦|èŠ±|æœ)$", "", tok)
    stripped = re.sub(r"^å°", "", stripped)  # å°å–¬æœ¨ -> å–¬æœ¨
    if stripped != tok and stripped in REVERSE_MAP:
        trait, canon = REVERSE_MAP[stripped]
        return trait, canon, f"strip:{stripped}"
    
    # 3. Contains matchï¼ˆç‰¹åˆ¥é‡å°ã€Œäº’ç”Ÿè‘‰åºã€ã€Œå°ç”Ÿè‘‰ã€é€™ç¨®ï¼‰
    leaf_arr_subs = {
        "alternate": ["äº’ç”Ÿ"],
        "opposite": ["å°ç”Ÿ"],
        "whorled": ["è¼ªç”Ÿ"],
        "basal": ["åŸºç”Ÿ", "è“®åº§", "å¢ç”Ÿ"]
    }
    for canon, subs in leaf_arr_subs.items():
        for s in subs:
            if s in tok:
                return "leaf_arrangement", canon, f"contains:{s}"
    
    return None


def suggest_rule(tok: str) -> Optional[Tuple[str, str, str]]:
    """
    è¦å‰‡å»ºè­°ï¼šæ ¹æ“šå¸¸è¦‹æ¨¡å¼æ¨æ–· trait é¡åˆ¥
    
    Returns:
        (trait, canonical, method) æˆ– None
    """
    tok = normalize_token(tok)
    if not tok:
        return None
    
    # leaf margin variants
    if "å…¨ç·£" in tok:
        return "leaf_margin", "entire", "rule:margin"
    if "é‹¸é½’" in tok:
        return "leaf_margin", "serrate", "rule:margin"
    if "æ³¢ç‹€" in tok:
        return "leaf_margin", "wavy", "rule:margin"
    if "è£‚" in tok and "è‘‰" in tok:
        return "leaf_margin", "lobed", "rule:margin"
    
    # texture
    if "é©è³ª" in tok:
        return "leaf_texture", "coriaceous", "rule:texture"
    if "ç´™è³ª" in tok:
        return "leaf_texture", "papery", "rule:texture"
    if "è‚‰è³ª" in tok:
        return "leaf_texture", "succulent", "rule:texture"
    
    # phenology
    if tok in ("è½è‘‰", "è½è‘‰æ€§"):
        return "phenology", "deciduous", "rule:phenology"
    if tok in ("å¸¸ç¶ ", "å¸¸ç¶ æ€§"):
        return "phenology", "evergreen", "rule:phenology"
    if tok == "åŠå¸¸ç¶ ":
        return "phenology", "semi_evergreen", "rule:phenology"
    
    # endemism
    if "ç‰¹æœ‰" in tok:
        return "endemism", "endemic", "rule:endemism"
    
    # reproductive system
    repro_map = {
        "é›Œé›„ç•°æ ª": "dioecious",
        "é›Œé›„åŒæ ª": "monoecious",
        "å…©æ€§èŠ±": "bisexual_flower",
        "å–®æ€§èŠ±": "unisexual_flower"
    }
    if tok in repro_map:
        return "reproductive_system", repro_map[tok], "rule:repro"
    
    # inflorescence
    if "èŠ±åº" in tok or tok in ("å–®ç”ŸèŠ±", "å–®ç”Ÿ"):
        if "ç¹–å½¢" in tok or "å‚˜å½¢" in tok:
            return "inflorescence", "umbel", "rule:infl"
        if "é ­ç‹€" in tok:
            return "inflorescence", "capitulum", "rule:infl"
        if "ç¹–æˆ¿" in tok:
            return "inflorescence", "corymb", "rule:infl"
        if "å–®ç”Ÿ" in tok:
            return "inflorescence", "solitary", "rule:infl"
    
    # fruit shape
    if "æœ" in tok:
        if "çƒå½¢" in tok:
            return "fruit_shape", "globose", "rule:fruit"
        if "åµå½¢" in tok and "æœ" in tok:
            return "fruit_shape", "ovoid", "rule:fruit"
        if "æ©¢åœ“" in tok and "æœ" in tok:
            return "fruit_shape", "ellipsoid", "rule:fruit"
    
    # leaf base
    if "åŸºéƒ¨" in tok:
        if "æ¥”å½¢" in tok:
            return "leaf_base", "cuneate", "rule:base"
        if "å¿ƒå½¢" in tok:
            return "leaf_base", "cordate", "rule:base"
        if "åœ“å½¢" in tok:
            return "leaf_base", "rounded", "rule:base"
    
    # special features
    if "æ°£ç”Ÿæ ¹" in tok or "æ°£æ ¹" in tok:
        return "special_features", "aerial_root", "rule:special"
    if "æ¿æ ¹" in tok:
        return "special_features", "buttress", "rule:special"
    if "èƒç”Ÿ" in tok:
        return "special_features", "viviparous", "rule:special"
    if "ç´…è‹" in tok or "è‹è‘‰" in tok:
        return "special_features", "bract_red", "rule:special"
    
    # é¡è‰² + å™¨å®˜ï¼ˆå¸¸è¦‹è®Šé«”ï¼‰
    color_organs = {
        "ç´…æœ": ("fruit_color", "red"),
        "é»ƒæœ": ("fruit_color", "yellow"),
        "é»‘æœ": ("fruit_color", "black"),
        "ç´«æœ": ("fruit_color", "purple"),
        "ç™½æœ": ("fruit_color", "white"),
        "ç´…è‹è‘‰": ("special_features", "bract_red"),
        "ç´…è‹": ("special_features", "bract_red"),
    }
    if tok in color_organs:
        trait, canon = color_organs[tok]
        return trait, canon, "rule:color_organ"
    
    # æœå¯¦å‹æ…‹
    fruit_types = {
        "çƒæœ": ("fruit_type", "cone"),
        "è’´æœ": ("fruit_type", "capsule"),
        "ç¿…æœ": ("fruit_type", "samara"),
        "ç˜¦æœ": ("fruit_type", "achene"),
        "é•·è§’æœ": ("fruit_type", "silique"),
    }
    if tok in fruit_types:
        trait, canon = fruit_types[tok]
        return trait, canon, "rule:fruit_type"
    
    return None


def parse_jsonl_line(line: str):
    """è§£æ JSONL è¡Œï¼ˆè™•ç†å°¾éš¨é€—è™Ÿï¼‰"""
    line = line.strip()
    if not line:
        return None
    if line.endswith(","):
        line = line[:-1]
    try:
        return json.loads(line)
    except json.JSONDecodeError:
        return None


# æƒæå…¨åº«ï¼šçµ±è¨ˆ token æ¬¡æ•¸
print(f"ğŸ“– è®€å–è³‡æ–™ï¼š{JSONL_PATH}")
counter = collections.Counter()
examples = collections.defaultdict(list)

with JSONL_PATH.open("r", encoding="utf-8") as f:
    for line_num, line in enumerate(f, 1):
        obj = parse_jsonl_line(line)
        if not obj:
            continue
        
        identification = obj.get("identification") or {}
        kf = identification.get("key_features") or []
        
        for t in kf:
            tok = normalize_token(t)
            if not tok or tok in ("æœªè¦‹æè¿°", "æœªè¦‹", "ä¸æ˜", "æœªæä¾›è³‡è¨Š"):
                continue
            
            counter[tok] += 1
            if len(examples[tok]) < 3:
                chinese_name = obj.get("chinese_name", "")
                if chinese_name:
                    examples[tok].append(chinese_name)

print(f"   ç¸½å…±æƒæåˆ° {len(counter)} å€‹å”¯ä¸€ token")

# é€ token åš mapping / å»ºè­°è£œè©
print(f"ğŸ” é–‹å§‹æ˜ å°„å’Œå»ºè­°...")
rows = []
unmapped = collections.Counter()

# patch çµæ§‹ï¼šæŠŠã€Œæ–°åŒç¾©è©ã€åŠ é€²ç¾æœ‰ VOCAB
patch = {"add_synonyms": {}}

def patch_add(trait, canon, synonym):
    """å°‡åŒç¾©è©åŠ å…¥ patch"""
    patch["add_synonyms"].setdefault(trait, {})
    patch["add_synonyms"][trait].setdefault(canon, [])
    if synonym not in patch["add_synonyms"][trait][canon]:
        patch["add_synonyms"][trait][canon].append(synonym)

for tok, cnt in counter.items():
    # å…ˆå˜—è©¦å•Ÿç™¼å¼æ˜ å°„
    result = heuristic_map(tok)
    if result is None:
        # å†å˜—è©¦è¦å‰‡å»ºè­°
        result = suggest_rule(tok)
    
    if result is None:
        unmapped[tok] = cnt
        rows.append([tok, cnt, "", "", "UNMAPPED", "|".join(examples[tok])])
        continue
    
    # mapped or suggested â†’ å¯«å…¥ patchï¼ˆæŠŠé€™å€‹ tok ç•¶åŒç¾©è©æ”¶é€²å»ï¼‰
    trait, canon, how = result
    patch_add(trait, canon, tok)
    rows.append([tok, cnt, trait, canon, how, "|".join(examples[tok])])

# è¼¸å‡ºå ±è¡¨
print(f"ğŸ“Š ç”¢ç”Ÿå ±è¡¨...")

# mapping report
with OUT_REPORT_CSV.open("w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["token", "count", "trait", "canonical", "how", "examples"])
    w.writerows(sorted(rows, key=lambda r: (-int(r[1]), r[0])))

# unmapped top
topN = 300
with OUT_UNMAPPED_CSV.open("w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["token", "count", "examples"])
    for tok, cnt in unmapped.most_common(topN):
        w.writerow([tok, cnt, "|".join(examples[tok])])

# patch json
OUT_PATCH_JSON.write_text(
    json.dumps(patch, ensure_ascii=False, indent=2),
    encoding="utf-8"
)

print(f"\nâœ… å®Œæˆï¼")
print(f"   ç¸½ token æ•¸ï¼š{sum(counter.values())}")
print(f"   å”¯ä¸€ tokenï¼š{len(counter)}")
print(f"   æœªæ˜ å°„å”¯ä¸€ tokenï¼š{len(unmapped)}")
print(f"   æ˜ å°„ç‡ï¼š{(len(counter) - len(unmapped)) / len(counter) * 100:.1f}%")
print(f"\nğŸ“ è¼¸å‡ºæª”æ¡ˆï¼š")
print(f"   - æ˜ å°„å ±è¡¨ï¼š{OUT_REPORT_CSV}")
print(f"   - æœªæ˜ å°„ Top {topN}ï¼š{OUT_UNMAPPED_CSV}")
print(f"   - å»ºè­° Patchï¼š{OUT_PATCH_JSON}")
print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
print(f"   1. æª¢æŸ¥ unmapped_top.csvï¼ŒæŸ¥çœ‹æœªæ˜ å°„çš„ token")
print(f"   2. æª¢æŸ¥ suggested_patch.jsonï¼Œç¢ºèªå»ºè­°æ˜¯å¦åˆç†")
print(f"   3. æ‰‹å‹•å¯©æ ¸å¾Œï¼Œåˆä½µ patch åˆ° trait_vocab.json")
