#!/usr/bin/env python3
"""
æ¸…ç†ä¸¦æ­£è¦åŒ–æ¤ç‰©è³‡æ–™
1. æ¸…ç† null æ¬„ä½
2. æ­£è¦åŒ– key_features
3. ç”Ÿæˆä¹¾æ·¨çš„ query_text_zhï¼ˆç”¨æ–¼ embeddingï¼‰
4. ç¢ºä¿ trait_tokens å®Œæ•´
"""

import json
import sys
from pathlib import Path
from typing import Dict, Any, List, Set
from normalize_features import normalize_features, load_normalize_rules

# è³‡æ–™æª”æ¡ˆè·¯å¾‘
DATA_DIR = Path(__file__).parent.parent / "data"
INPUT_FILE = DATA_DIR / "plants-forest-gov-tw-enhanced.jsonl"
OUTPUT_FILE = DATA_DIR / "plants-forest-gov-tw-clean.jsonl"
BACKUP_FILE = DATA_DIR / "plants-forest-gov-tw-enhanced.jsonl.backup"

# Must traitsï¼ˆé«˜ä¿¡å¿ƒåº¦ä¸”é—œéµçš„ç‰¹å¾µï¼‰
MUST_TRAITS = {
    "life_form",  # ç”Ÿæ´»å‹ï¼ˆå–¬æœ¨/çŒæœ¨/è‰æœ¬ï¼‰æ˜¯é—œéµ
    "leaf_arrangement",  # è‘‰åºï¼ˆäº’ç”Ÿ/å°ç”Ÿï¼‰æ˜¯é—œéµ
}

def clean_null_fields(plant: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ¸…ç† null æ¬„ä½
    ç§»é™¤æ‰€æœ‰å€¼ç‚º null çš„æ¬„ä½
    """
    def clean_dict(d: Any) -> Any:
        if isinstance(d, dict):
            return {k: clean_dict(v) for k, v in d.items() if v is not None}
        elif isinstance(d, list):
            return [clean_dict(item) for item in d if item is not None]
        else:
            return d
    
    return clean_dict(plant)

def build_query_text_zh(plant: Dict[str, Any]) -> str:
    """
    æ§‹å»ºç°¡çŸ­çš„ query_text_zhï¼ˆåªç”¨æ–¼ embeddingï¼‰
    
    åªåŒ…å«ï¼š
    - 1-2 å¥ç°¡æ½”çš„å½¢æ…‹æè¿°
    - é—œéµç‰¹å¾µï¼ˆæ­£è¦åŒ–å¾Œï¼‰
    
    çµ•å°ä¸åŒ…å«ï¼š
    - æ­¥é©Ÿæ–‡å­—ï¼ˆç¬¬ä¸€æ­¥ã€ç¬¬äºŒæ­¥...ï¼‰
    - ä¸ç¢ºå®šèªå¥ï¼ˆæ¨æ¸¬ã€ä¼°è¨ˆ...ï¼‰
    - æµç¨‹è©
    """
    parts = []
    
    # 1. ç”Ÿæ´»å‹ï¼ˆæœ€é‡è¦ï¼‰
    identification = plant.get("identification", {})
    life_form = identification.get("life_form")
    if life_form:
        parts.append(life_form)
    
    # 2. å½¢æ…‹æ‘˜è¦ï¼ˆå¦‚æœå­˜åœ¨ä¸”ç°¡æ½”ï¼‰
    morphology_summary = identification.get("morphology_summary_zh", "")
    if morphology_summary:
        # åªå–å‰ 100 å­—ï¼Œç¢ºä¿ç°¡æ½”
        summary_clean = morphology_summary[:100].strip()
        if summary_clean:
            parts.append(summary_clean)
    
    # 3. é—œéµç‰¹å¾µï¼ˆæ­£è¦åŒ–å¾Œï¼Œæœ€å¤š 10 å€‹ï¼‰
    key_features = identification.get("key_features", [])
    if key_features:
        # å±•å¹³åµŒå¥—åˆ—è¡¨
        flattened_features = []
        for item in key_features:
            if isinstance(item, str):
                flattened_features.append(item)
            elif isinstance(item, list):
                flattened_features.extend([str(x) for x in item if x])
            else:
                flattened_features.append(str(item))
        
        rules = load_normalize_rules()
        normalized = normalize_features(flattened_features, rules)
        # åªå–å‰ 10 å€‹æœ€é‡è¦çš„ï¼Œç¢ºä¿éƒ½æ˜¯å­—ç¬¦ä¸²
        normalized_clean = [str(n) for n in normalized[:10] if n]
        key_features_text = " ".join(normalized_clean)
        if key_features_text:
            parts.append(key_features_text)
    
    # çµ„åˆï¼ˆç”¨å¥è™Ÿåˆ†éš”ï¼Œç¢ºä¿ç°¡æ½”ï¼‰
    # ç¢ºä¿æ‰€æœ‰ parts éƒ½æ˜¯å­—ç¬¦ä¸²
    parts_clean = [str(p) for p in parts if p]
    query_text = "ã€‚".join(parts_clean)
    
    # é™åˆ¶é•·åº¦ï¼ˆæœ€å¤š 200 å­—å…ƒï¼‰
    if len(query_text) > 200:
        query_text = query_text[:200]
    
    return query_text.strip()

def extract_must_traits(trait_tokens: List[str]) -> List[str]:
    """
    å¾ trait_tokens ä¸­æå– must traitsï¼ˆé«˜ä¿¡å¿ƒåº¦ä¸”é—œéµï¼‰
    """
    must = []
    for token in trait_tokens:
        if "=" in token:
            trait, value = token.split("=", 1)
            if trait in MUST_TRAITS:
                must.append(token)
    return must

def enhance_plant(plant: Dict[str, Any]) -> Dict[str, Any]:
    """
    å¢å¼·æ¤ç‰©è³‡æ–™
    """
    # 1. æ¸…ç† null æ¬„ä½
    plant = clean_null_fields(plant)
    
    # 2. æ­£è¦åŒ– key_features
    identification = plant.get("identification", {})
    key_features = identification.get("key_features", [])
    if key_features:
        # å±•å¹³åµŒå¥—åˆ—è¡¨ï¼ˆè™•ç†æŸäº›è³‡æ–™ä¸­ key_features å¯èƒ½æ˜¯åµŒå¥—åˆ—è¡¨çš„æƒ…æ³ï¼‰
        flattened_features = []
        for item in key_features:
            if isinstance(item, str):
                flattened_features.append(item)
            elif isinstance(item, list):
                flattened_features.extend([str(x) for x in item if x])
            else:
                flattened_features.append(str(item))
        
        rules = load_normalize_rules()
        key_features_norm = normalize_features(flattened_features, rules)
        identification["key_features_norm"] = key_features_norm
        plant["identification"] = identification
    
    # 3. ç”Ÿæˆ query_text_zh
    query_text_zh = build_query_text_zh(plant)
    identification["query_text_zh"] = query_text_zh
    plant["identification"] = identification
    
    # 4. æå– must traits
    trait_tokens = identification.get("trait_tokens", [])
    if trait_tokens:
        must_traits = extract_must_traits(trait_tokens)
        identification["must_traits"] = must_traits
        plant["identification"] = identification
    
    return plant

def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 60)
    print("ğŸ§¹ æ¸…ç†ä¸¦æ­£è¦åŒ–æ¤ç‰©è³‡æ–™")
    print("=" * 60)
    
    if not INPUT_FILE.exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ: {INPUT_FILE}")
        sys.exit(1)
    
    # å‚™ä»½åŸæª”æ¡ˆ
    print(f"ğŸ“¦ å‚™ä»½åŸæª”æ¡ˆ: {BACKUP_FILE}")
    import shutil
    shutil.copy2(INPUT_FILE, BACKUP_FILE)
    
    # è¼‰å…¥è³‡æ–™
    print(f"ğŸ“‚ è¼‰å…¥è³‡æ–™: {INPUT_FILE}")
    plants = []
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                plant = json.loads(line)
                plants.append(plant)
            except json.JSONDecodeError as e:
                print(f"âš ï¸ è·³éç„¡æ•ˆ JSON: {e}")
                continue
    
    print(f"âœ… è¼‰å…¥ {len(plants)} ç­†è³‡æ–™")
    
    # è™•ç†æ¯ç­†è³‡æ–™
    print("ğŸ”§ è™•ç†è³‡æ–™...")
    cleaned_plants = []
    stats = {
        "total": len(plants),
        "cleaned": 0,
        "with_query_text": 0,
        "with_norm_features": 0,
        "with_must_traits": 0
    }
    
    for i, plant in enumerate(plants):
        try:
            enhanced = enhance_plant(plant)
            cleaned_plants.append(enhanced)
            stats["cleaned"] += 1
            
            if enhanced.get("identification", {}).get("query_text_zh"):
                stats["with_query_text"] += 1
            if enhanced.get("identification", {}).get("key_features_norm"):
                stats["with_norm_features"] += 1
            if enhanced.get("identification", {}).get("must_traits"):
                stats["with_must_traits"] += 1
            
            if (i + 1) % 500 == 0:
                print(f"  è™•ç†é€²åº¦: {i + 1}/{len(plants)}")
        except Exception as e:
            print(f"âš ï¸ è™•ç†ç¬¬ {i + 1} ç­†è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    
    # å¯«å…¥è¼¸å‡ºæª”æ¡ˆ
    print(f"ğŸ’¾ å¯«å…¥è¼¸å‡ºæª”æ¡ˆ: {OUTPUT_FILE}")
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for plant in cleaned_plants:
            f.write(json.dumps(plant, ensure_ascii=False) + '\n')
    
    # çµ±è¨ˆå ±å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š è™•ç†çµ±è¨ˆ")
    print("=" * 60)
    print(f"ç¸½ç­†æ•¸: {stats['total']}")
    print(f"æˆåŠŸæ¸…ç†: {stats['cleaned']}")
    print(f"æœ‰ query_text_zh: {stats['with_query_text']}")
    print(f"æœ‰æ­£è¦åŒ–ç‰¹å¾µ: {stats['with_norm_features']}")
    print(f"æœ‰ must_traits: {stats['with_must_traits']}")
    print("\nâœ… å®Œæˆï¼")
    print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆ: {OUTPUT_FILE}")
    print(f"ğŸ“¦ å‚™ä»½æª”æ¡ˆ: {BACKUP_FILE}")

if __name__ == "__main__":
    main()
