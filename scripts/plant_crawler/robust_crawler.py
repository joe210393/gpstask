#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆçˆ¬è™« - é˜²æ­¢ä¸­æ–­å’Œè‡ªåŠ¨æ¢å¤
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import sys
from pathlib import Path
from datetime import datetime
import signal
import os


class RobustCrawler:
    """ç¨³å¥çš„çˆ¬è™«ï¼Œèƒ½å¤„ç†ä¸­æ–­å’Œè‡ªåŠ¨æ¢å¤"""

    def __init__(self, codes_file='plant_codes.txt', output_dir='./plant_data', delay=1.5):
        self.base_url = "https://tai2.ntu.edu.tw"
        self.codes_file = codes_file
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.delay = delay

        # è®¾ç½® requests session
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Educational Research Bot)',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8'
        })

        # ç»Ÿè®¡
        self.total = 0
        self.completed = 0
        self.skipped = 0
        self.failed = 0
        self.start_time = time.time()

        # æ—¥å¿—å’ŒçŠ¶æ€æ–‡ä»¶
        self.log_file = self.output_dir / 'crawler.log'
        self.status_file = self.output_dir / 'crawler_status.json'
        self.failed_codes_file = self.output_dir / 'failed_codes.txt'

        # å¤„ç†ä¸­æ–­ä¿¡å·
        signal.signal(signal.SIGINT, self.handle_interrupt)
        signal.signal(signal.SIGTERM, self.handle_interrupt)

    def log(self, message, level='INFO'):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_msg = f"[{timestamp}] [{level}] {message}"

        # å†™å…¥æ–‡ä»¶
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_msg + '\n')

        # è¾“å‡ºåˆ°æ§åˆ¶å°
        print(log_msg)
        sys.stdout.flush()

    def handle_interrupt(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·"""
        self.log("âš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜çŠ¶æ€...", 'WARN')
        self.save_status('interrupted')
        self.log(f"âœ… çŠ¶æ€å·²ä¿å­˜ã€‚å·²å®Œæˆ: {self.completed}/{self.total}", 'INFO')
        sys.exit(0)

    def get_plant_data(self, code: str, max_retries=5):
        """è·å–æ¤ç‰©æ•°æ®ï¼Œå¢å¼ºé‡è¯•æœºåˆ¶"""
        for attempt in range(max_retries):
            try:
                url = f"{self.base_url}/PlantInfo/species-name.php?code={code}"
                response = self.session.get(url, timeout=30)  # å¢åŠ è¶…æ—¶æ—¶é—´

                if response.status_code != 200:
                    if attempt < max_retries - 1:
                        wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿: 1, 2, 4, 8, 16ç§’
                        self.log(f"âš ï¸  HTTP {response.status_code} [{code}] - ç­‰å¾… {wait_time}s åé‡è¯• ({attempt+1}/{max_retries})", 'WARN')
                        time.sleep(wait_time)
                        continue
                    return None

                soup = BeautifulSoup(response.text, 'html.parser')

                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆé¡µé¢
                title = soup.find('title')
                if title and 'Search-å°ç£æ¤ç‰©è³‡è¨Šæ•´åˆæŸ¥è©¢ç³»çµ±' in title.text:
                    return None

                # æå–æ•°æ®
                data = self.parse_plant_page(soup, code)
                return data

            except requests.exceptions.Timeout:
                wait_time = 5 * (attempt + 1)
                self.log(f"â±ï¸  è¶…æ—¶ [{code}] - ç­‰å¾… {wait_time}s åé‡è¯• ({attempt+1}/{max_retries})", 'WARN')
                time.sleep(wait_time)

            except requests.exceptions.ConnectionError as e:
                wait_time = 10 * (attempt + 1)
                self.log(f"ğŸ”Œ è¿æ¥é”™è¯¯ [{code}] - {str(e)[:50]} - ç­‰å¾… {wait_time}s åé‡è¯• ({attempt+1}/{max_retries})", 'ERROR')
                time.sleep(wait_time)

            except Exception as e:
                self.log(f"âŒ æœªçŸ¥é”™è¯¯ [{code}] - {str(e)}", 'ERROR')
                if attempt < max_retries - 1:
                    time.sleep(5)
                    continue
                return None

        return None

    def parse_plant_page(self, soup, code):
        """è§£ææ¤ç‰©é¡µé¢ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        data = {
            'code': code,
            'url': f"{self.base_url}/PlantInfo/species-name.php?code={code}",
            'crawled_at': datetime.now().isoformat()
        }

        # æå–æ ‡é¢˜/åç§°
        title = soup.find('title')
        if title:
            data['title'] = title.text.strip()

        # æå–å­¦å
        name_tag = soup.find('span', class_='name')
        if name_tag:
            data['scientific_name'] = name_tag.text.strip()

        # æå–æè¿°
        desc_tag = soup.find('div', class_='description')
        if desc_tag:
            data['description'] = desc_tag.text.strip()

        return data

    def save_plant_data(self, code, data):
        """ä¿å­˜æ¤ç‰©æ•°æ®"""
        # å°† + è½¬æ¢ä¸º _
        safe_code = code.replace('+', '_')
        file_path = self.output_dir / f"{safe_code}.json"

        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def save_status(self, status='running'):
        """ä¿å­˜å½“å‰çŠ¶æ€"""
        elapsed = time.time() - self.start_time
        data = {
            'status': status,
            'total': self.total,
            'completed': self.completed,
            'skipped': self.skipped,
            'failed': self.failed,
            'progress': f"{self.completed}/{self.total}",
            'percentage': round(self.completed / self.total * 100, 2) if self.total > 0 else 0,
            'elapsed_seconds': round(elapsed, 2),
            'elapsed_hours': round(elapsed / 3600, 2),
            'updated_at': datetime.now().isoformat()
        }

        with open(self.status_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load_codes(self):
        """åŠ è½½æ¤ç‰©ç¼–ç åˆ—è¡¨"""
        with open(self.codes_file, 'r', encoding='utf-8') as f:
            codes = [line.strip() for line in f if line.strip()]
        return codes

    def get_completed_codes(self):
        """è·å–å·²å®Œæˆçš„ç¼–ç """
        completed = set()
        if not self.output_dir.exists():
            return completed

        for file_path in self.output_dir.glob('*.json'):
            if file_path.name in ['crawler_status.json', 'all_plants.json']:
                continue
            # ä»æ–‡ä»¶åè¿˜åŸç¼–ç 
            code = file_path.stem.replace('_', '+')
            completed.add(code)

        return completed

    def crawl_all(self):
        """çˆ¬å–æ‰€æœ‰æ¤ç‰©"""
        self.log("=" * 80)
        self.log("ğŸš€ å¯åŠ¨ç¨³å¥çˆ¬è™«")
        self.log("=" * 80)

        # åŠ è½½ç¼–ç 
        all_codes = self.load_codes()
        self.total = len(all_codes)
        self.log(f"ğŸ“ æ€»å…± {self.total} ä¸ªæ¤ç‰©ç¼–ç ")

        # è·å–å·²å®Œæˆçš„
        completed_codes = self.get_completed_codes()
        self.skipped = len(completed_codes)
        self.log(f"âœ… å·²å®Œæˆ {self.skipped} ä¸ªï¼Œè·³è¿‡")

        # å¾…å¤„ç†çš„
        remaining_codes = [c for c in all_codes if c not in completed_codes]
        self.log(f"ğŸ“‹ å¾…çˆ¬å– {len(remaining_codes)} ä¸ª")
        self.log(f"â±ï¸  é¢„è®¡è€—æ—¶: {len(remaining_codes) * self.delay / 60:.1f} åˆ†é’Ÿ")
        self.log("=" * 80)

        # å¼€å§‹çˆ¬å–
        failed_codes = []

        for i, code in enumerate(remaining_codes, 1):
            self.log(f"\n[{i}/{len(remaining_codes)}] çˆ¬å–: {code}")

            # è·å–æ•°æ®
            data = self.get_plant_data(code)

            if data:
                self.save_plant_data(code, data)
                self.completed += 1
                self.log(f"  âœ… æˆåŠŸä¿å­˜", 'SUCCESS')
            else:
                self.failed += 1
                failed_codes.append(code)
                self.log(f"  âŒ å¤±è´¥", 'ERROR')

            # æ¯10ä¸ªä¿å­˜ä¸€æ¬¡çŠ¶æ€
            if i % 10 == 0:
                self.save_status('running')
                self.log(f"ğŸ“Š è¿›åº¦: {self.completed + self.skipped}/{self.total} ({(self.completed + self.skipped)/self.total*100:.1f}%)")

            # å»¶è¿Ÿ
            time.sleep(self.delay)

        # ä¿å­˜å¤±è´¥çš„ç¼–ç 
        if failed_codes:
            with open(self.failed_codes_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(failed_codes))
            self.log(f"\nâš ï¸  {len(failed_codes)} ä¸ªå¤±è´¥çš„ç¼–ç å·²ä¿å­˜åˆ°: {self.failed_codes_file}")

        # å®Œæˆ
        self.save_status('completed')
        elapsed = time.time() - self.start_time

        self.log("\n" + "=" * 80)
        self.log("ğŸ‰ çˆ¬å–å®Œæˆï¼")
        self.log(f"âœ… æˆåŠŸ: {self.completed} ä¸ª")
        self.log(f"â­ï¸  è·³è¿‡: {self.skipped} ä¸ªï¼ˆå·²å­˜åœ¨ï¼‰")
        self.log(f"âŒ å¤±è´¥: {self.failed} ä¸ª")
        self.log(f"ğŸ“Š æ€»è®¡: {self.completed + self.skipped}/{self.total}")
        self.log(f"â±ï¸  æ€»è€—æ—¶: {elapsed/3600:.2f} å°æ—¶")
        self.log("=" * 80)


def main():
    codes_file = 'plant_codes.txt'
    delay = 1.5

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        # ç¬¬ä¸€ä¸ªå‚æ•°å¯ä»¥æ˜¯æ–‡ä»¶åæˆ–å»¶è¿Ÿæ—¶é—´
        arg1 = sys.argv[1]
        if arg1.endswith('.txt'):
            codes_file = arg1
            # ç¬¬äºŒä¸ªå‚æ•°æ˜¯å»¶è¿Ÿæ—¶é—´
            if len(sys.argv) > 2:
                try:
                    delay = float(sys.argv[2])
                except:
                    pass
        else:
            # ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å»¶è¿Ÿæ—¶é—´
            try:
                delay = float(arg1)
            except:
                pass

    print(f"ğŸ“ ä½¿ç”¨ç¼–ç æ–‡ä»¶: {codes_file}")
    print(f"â±ï¸  è¯·æ±‚å»¶è¿Ÿ: {delay} ç§’")
    print()

    crawler = RobustCrawler(codes_file=codes_file, delay=delay)
    crawler.crawl_all()


if __name__ == '__main__':
    main()
