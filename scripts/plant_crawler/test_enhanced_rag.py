#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦å¢å¼·ç‰ˆ RAG è³‡æ–™å“è³ª
æ¯”è¼ƒåŸå§‹ç‰ˆæœ¬èˆ‡å¢å¼·ç‰ˆæœ¬çš„å·®ç•°
"""

import json
from pathlib import Path


def test_enhanced_rag():
    """æ¸¬è©¦å¢å¼·ç‰ˆ RAG è³‡æ–™"""

    print("=" * 70)
    print("å¢å¼·ç‰ˆ RAG è³‡æ–™å“è³ªæ¸¬è©¦")
    print("=" * 70)

    # è¼‰å…¥åŸå§‹ç‰ˆæœ¬
    original_file = Path('./plant_data/rag_documents.json')
    enhanced_file = Path('./plant_data/rag_documents_enhanced.json')

    if not original_file.exists() or not enhanced_file.exists():
        print("âŒ æ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆ")
        return

    with open(original_file, 'r', encoding='utf-8') as f:
        original_data = json.load(f)

    with open(enhanced_file, 'r', encoding='utf-8') as f:
        enhanced_data = json.load(f)

    original_docs = original_data.get('documents', [])
    enhanced_docs = enhanced_data.get('documents', [])

    print(f"\nğŸ“Š åŸºæœ¬çµ±è¨ˆæ¯”è¼ƒ")
    print(f"{'é …ç›®':<30} {'åŸå§‹ç‰ˆæœ¬':<15} {'å¢å¼·ç‰ˆæœ¬':<15} {'æ”¹é€²':<10}")
    print("-" * 70)

    # 1. æ–‡ä»¶æ•¸é‡
    print(f"{'æ–‡ä»¶æ•¸é‡':<30} {len(original_docs):<15} {len(enhanced_docs):<15} {'=':<10}")

    # 2. å¹³å‡æ–‡æœ¬é•·åº¦
    orig_avg_len = sum(len(d['text']) for d in original_docs) / len(original_docs)
    enh_avg_len = sum(len(d['text']) for d in enhanced_docs) / len(enhanced_docs)
    improvement = (enh_avg_len - orig_avg_len) / orig_avg_len * 100

    print(f"{'å¹³å‡æ–‡æœ¬é•·åº¦ï¼ˆå­—å…ƒï¼‰':<30} {orig_avg_len:<15.0f} {enh_avg_len:<15.0f} {improvement:>+.1f}%")

    # 3. çµæ§‹åŒ–ç¨‹åº¦
    print(f"\nğŸ“‹ æ–°å¢åŠŸèƒ½çµ±è¨ˆ")
    print("-" * 70)

    has_guide = sum(1 for d in enhanced_docs if d.get('identification_guide'))
    has_features = sum(1 for d in enhanced_docs if d.get('key_features'))
    avg_features = sum(len(d.get('key_features', [])) for d in enhanced_docs) / len(enhanced_docs)

    print(f"æœ‰è­˜åˆ¥æŒ‡å—çš„æ–‡ä»¶ï¼š{has_guide}/{len(enhanced_docs)} ({has_guide/len(enhanced_docs)*100:.1f}%)")
    print(f"æœ‰é—œéµç‰¹å¾µçš„æ–‡ä»¶ï¼š{has_features}/{len(enhanced_docs)} ({has_features/len(enhanced_docs)*100:.1f}%)")
    print(f"å¹³å‡é—œéµç‰¹å¾µæ•¸ï¼š{avg_features:.1f} å€‹/æ¤ç‰©")

    # 4. è­˜åˆ¥æŒ‡å—è¦†è“‹ç‡çµ±è¨ˆ
    print(f"\nğŸ” è­˜åˆ¥æŒ‡å—ç‰¹å¾µè¦†è“‹ç‡")
    print("-" * 70)

    feature_types = {
        'è‘‰': 0, 'èŠ±': 0, 'æœ': 0, 'è–': 0, 'æ ¹': 0,
        'æ¯›è¢«': 0, 'è³ªåœ°': 0, 'æ¨¹çš®': 0
    }

    for doc in enhanced_docs:
        guide = doc.get('identification_guide', '')
        for feature_type in feature_types.keys():
            if f'ã€{feature_type}ã€‘' in guide:
                feature_types[feature_type] += 1

    for feature_type, count in sorted(feature_types.items(), key=lambda x: x[1], reverse=True):
        if count > 0:
            percentage = count / len(enhanced_docs) * 100
            print(f"{feature_type}ï¼š{count}/{len(enhanced_docs)} ({percentage:.1f}%)")

    # 5. å°æ¯”ç¯„ä¾‹
    print(f"\nğŸ“„ å°æ¯”ç¯„ä¾‹")
    print("=" * 70)

    # é¸æ“‡ä¸€å€‹æœ‰è±å¯Œç‰¹å¾µçš„æ–‡ä»¶
    sample_id = enhanced_docs[2]['id']  # ç¬¬ä¸‰å€‹æ–‡ä»¶

    orig_doc = next(d for d in original_docs if d['id'] == sample_id)
    enh_doc = next(d for d in enhanced_docs if d['id'] == sample_id)

    print(f"\næ¤ç‰©ï¼š{enh_doc['metadata']['name_zh']} ({enh_doc['metadata']['name_latin']})")

    print(f"\nã€åŸå§‹ç‰ˆæœ¬ã€‘æ–‡æœ¬é•·åº¦ï¼š{len(orig_doc['text'])} å­—å…ƒ")
    print("-" * 70)
    print(orig_doc['text'][:300] + "..." if len(orig_doc['text']) > 300 else orig_doc['text'])

    print(f"\nã€å¢å¼·ç‰ˆæœ¬ã€‘æ–‡æœ¬é•·åº¦ï¼š{len(enh_doc['text'])} å­—å…ƒ")
    print("-" * 70)
    print(enh_doc['text'][:500] + "..." if len(enh_doc['text']) > 500 else enh_doc['text'])

    print("\n" + "-" * 70)
    print("æ–°å¢è­˜åˆ¥æŒ‡å—ï¼š")
    print(enh_doc.get('identification_guide', 'ï¼ˆç„¡ï¼‰'))

    print("\næ–°å¢é—œéµç‰¹å¾µï¼š")
    for i, feature in enumerate(enh_doc.get('key_features', []), 1):
        print(f"  {i}. {feature}")

    # 6. å“è³ªè©•åˆ†
    print(f"\n" + "=" * 70)
    print("å“è³ªè©•åˆ†")
    print("=" * 70)

    score = 0
    max_score = 6

    # è©•åˆ†æ¨™æº–
    if has_guide == len(enhanced_docs):
        score += 1
        print("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½æœ‰è­˜åˆ¥æŒ‡å—")
    else:
        print(f"âš ï¸  è­˜åˆ¥æŒ‡å—è¦†è“‹ç‡ï¼š{has_guide/len(enhanced_docs)*100:.1f}%")

    if has_features == len(enhanced_docs):
        score += 1
        print("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½æœ‰é—œéµç‰¹å¾µ")
    else:
        print(f"âš ï¸  é—œéµç‰¹å¾µè¦†è“‹ç‡ï¼š{has_features/len(enhanced_docs)*100:.1f}%")

    if avg_features >= 3:
        score += 1
        print(f"âœ… å¹³å‡é—œéµç‰¹å¾µæ•¸å……è¶³ï¼ˆ{avg_features:.1f}ï¼‰")
    else:
        print(f"âš ï¸  å¹³å‡é—œéµç‰¹å¾µæ•¸åå°‘ï¼ˆ{avg_features:.1f}ï¼‰")

    if feature_types['è‘‰'] > 0:
        score += 1
        print(f"âœ… æœ‰è‘‰ç‰¹å¾µè³‡è¨Šï¼ˆ{feature_types['è‘‰']} å€‹ï¼‰")

    if feature_types['è–'] > 0:
        score += 1
        print(f"âœ… æœ‰è–ç‰¹å¾µè³‡è¨Šï¼ˆ{feature_types['è–']} å€‹ï¼‰")

    if enh_avg_len > orig_avg_len:
        score += 1
        print(f"âœ… æ–‡æœ¬é•·åº¦å¢åŠ ï¼ˆ+{improvement:.1f}%ï¼‰")

    print(f"\næœ€çµ‚è©•åˆ†ï¼š{score}/{max_score}")

    if score >= 5:
        print("ğŸ‰ å¢å¼·ç‰ˆ RAG è³‡æ–™å“è³ªå„ªç§€ï¼")
        print("\nâœ… ä¸»è¦æ”¹é€²ï¼š")
        print("1. æ–°å¢çµæ§‹åŒ–è­˜åˆ¥æŒ‡å—ï¼ˆè‘‰ã€èŠ±ã€æœã€è–ã€æ ¹ï¼‰")
        print("2. è‡ªå‹•æå–é—œéµç‰¹å¾µï¼ˆå½¢ç‹€ã€é¡è‰²ã€å°ºå¯¸ï¼‰")
        print("3. æ–‡æœ¬é•·åº¦å¢åŠ ï¼Œè³‡è¨Šæ›´è±å¯Œ")
        print("4. æ›´é©åˆæ¤ç‰©è­˜åˆ¥æ‡‰ç”¨")
    elif score >= 4:
        print("âš ï¸  å¢å¼·ç‰ˆ RAG è³‡æ–™å“è³ªè‰¯å¥½ï¼Œä»æœ‰æ”¹é€²ç©ºé–“")
    else:
        print("âŒ å¢å¼·ç‰ˆ RAG è³‡æ–™éœ€è¦é€²ä¸€æ­¥æ”¹é€²")

    print("\n" + "=" * 70)
    print("å»ºè­°ä½¿ç”¨æ–¹å¼")
    print("=" * 70)
    print("\n1. ä½¿ç”¨å¢å¼·ç‰ˆè³‡æ–™é€²è¡Œæ¤ç‰©è­˜åˆ¥")
    print("   - ä¸­æ–‡æŸ¥è©¢ï¼šã€Œæ©¢åœ“å½¢è‘‰å­ã€â†’ åŒ¹é… ã€è‘‰ã€‘å½¢ç‹€è³‡è¨Š")
    print("   - è‹±æ–‡æŸ¥è©¢ï¼šã€Œlanceolate leavesã€â†’ åŒ¹é…è©³ç´°æè¿°")
    print("   - æ··åˆæŸ¥è©¢ï¼šã€Œç¶ è‰²çš„è–ã€â†’ åŒ¹é… ã€è–ã€‘é¡è‰²è³‡è¨Š")

    print("\n2. å‘é‡æ¨¡å‹å»ºè­°")
    print("   - jinaai/jina-embeddings-v2-base-zhï¼ˆè·¨èªè¨€ï¼‰")
    print("   - æˆ– text-embedding-3-smallï¼ˆOpenAIï¼‰")

    print("\n3. RAG æª¢ç´¢ç­–ç•¥")
    print("   - å…ˆç”¨å‘é‡æœå°‹æ‰¾åˆ°å€™é¸æ¤ç‰©ï¼ˆTop 5-10ï¼‰")
    print("   - æ ¹æ“šã€Œè­˜åˆ¥æŒ‡å—ã€é€²è¡ŒäºŒæ¬¡ç¯©é¸")
    print("   - ä½¿ç”¨ã€Œé—œéµç‰¹å¾µã€ç”Ÿæˆå›ç­”")

    print("\n" + "=" * 70)


def compare_specific_examples():
    """æ¯”è¼ƒå…·é«”ç¯„ä¾‹"""
    print("\n" + "=" * 70)
    print("è©³ç´°ç¯„ä¾‹å°æ¯”")
    print("=" * 70)

    enhanced_file = Path('./plant_data/rag_documents_enhanced.jsonl')

    if not enhanced_file.exists():
        return

    with open(enhanced_file, 'r', encoding='utf-8') as f:
        docs = [json.loads(line) for line in f]

    # æ‰¾å‡ºç‰¹å¾µæœ€è±å¯Œçš„ 3 å€‹
    sorted_docs = sorted(docs, key=lambda d: len(d.get('identification_guide', '')), reverse=True)

    print(f"\nç‰¹å¾µæœ€è±å¯Œçš„ 3 å€‹æ¤ç‰©ï¼š\n")

    for i, doc in enumerate(sorted_docs[:3], 1):
        print(f"{i}. {doc['metadata']['name_zh']} ({doc['metadata']['name_latin']})")
        print(f"   è­˜åˆ¥æŒ‡å—é•·åº¦ï¼š{len(doc.get('identification_guide', ''))} å­—å…ƒ")
        print(f"   é—œéµç‰¹å¾µæ•¸ï¼š{len(doc.get('key_features', []))} å€‹")

        guide = doc.get('identification_guide', '')
        if guide:
            # çµ±è¨ˆç‰¹å¾µé¡å‹
            types = []
            for t in ['è‘‰', 'èŠ±', 'æœ', 'è–', 'æ ¹', 'æ¯›è¢«', 'è³ªåœ°']:
                if f'ã€{t}ã€‘' in guide:
                    types.append(t)
            print(f"   ç‰¹å¾µé¡å‹ï¼š{', '.join(types)}")

        print()


if __name__ == '__main__':
    test_enhanced_rag()
    compare_specific_examples()
