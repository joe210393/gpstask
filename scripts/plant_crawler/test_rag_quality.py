#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦ RAG è³‡æ–™å“è³ª
æª¢æŸ¥ RAG æ–‡ä»¶çš„å®Œæ•´æ€§å’Œå¯ç”¨æ€§
"""

import json
from pathlib import Path


def test_rag_documents():
    """æ¸¬è©¦ RAG æ–‡ä»¶å“è³ª"""

    print("=" * 70)
    print("RAG è³‡æ–™å“è³ªæ¸¬è©¦")
    print("=" * 70)

    # è¼‰å…¥è³‡æ–™
    rag_file = Path('./plant_data/rag_documents.json')
    if not rag_file.exists():
        print("âŒ æ‰¾ä¸åˆ° rag_documents.json")
        return

    with open(rag_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    documents = data.get('documents', [])
    metadata = data.get('metadata', {})

    print(f"\nğŸ“‹ åŸºæœ¬è³‡è¨Šï¼š")
    print(f"   ç¸½æ–‡ä»¶æ•¸ï¼š{metadata.get('total_documents', 0)}")
    print(f"   å»ºç«‹æ™‚é–“ï¼š{metadata.get('created_at', 'N/A')}")

    # æ¸¬è©¦ 1ï¼šæª¢æŸ¥æ–‡ä»¶çµæ§‹
    print(f"\nğŸ” æ¸¬è©¦ 1ï¼šæ–‡ä»¶çµæ§‹æª¢æŸ¥")
    issues = []

    for i, doc in enumerate(documents, 1):
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        if 'id' not in doc:
            issues.append(f"æ–‡ä»¶ {i} ç¼ºå°‘ id")
        if 'text' not in doc:
            issues.append(f"æ–‡ä»¶ {i} ç¼ºå°‘ text")
        if 'metadata' not in doc:
            issues.append(f"æ–‡ä»¶ {i} ç¼ºå°‘ metadata")

        # æª¢æŸ¥æ–‡æœ¬ä¸ç‚ºç©º
        if doc.get('text', '').strip() == '':
            issues.append(f"æ–‡ä»¶ {i} ({doc.get('id', 'unknown')}) æ–‡æœ¬ç‚ºç©º")

    if issues:
        print(f"   âŒ ç™¼ç¾ {len(issues)} å€‹å•é¡Œï¼š")
        for issue in issues[:5]:  # åªé¡¯ç¤ºå‰ 5 å€‹
            print(f"      - {issue}")
    else:
        print(f"   âœ… æ‰€æœ‰æ–‡ä»¶çµæ§‹å®Œæ•´")

    # æ¸¬è©¦ 2ï¼šæ–‡æœ¬å“è³ªåˆ†æ
    print(f"\nğŸ“Š æ¸¬è©¦ 2ï¼šæ–‡æœ¬å“è³ªåˆ†æ")

    text_lengths = [len(doc['text']) for doc in documents]
    avg_length = sum(text_lengths) / len(text_lengths) if text_lengths else 0

    # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„æè¿°æ–‡æœ¬
    short_texts = [doc for doc in documents if len(doc.get('text', '')) < 100]
    long_texts = [doc for doc in documents if len(doc.get('text', '')) > 500]

    print(f"   å¹³å‡æ–‡æœ¬é•·åº¦ï¼š{avg_length:.0f} å­—å…ƒ")
    print(f"   éçŸ­æ–‡æœ¬ï¼ˆ<100 å­—å…ƒï¼‰ï¼š{len(short_texts)} å€‹")
    print(f"   è‰¯å¥½é•·åº¦ï¼ˆ>500 å­—å…ƒï¼‰ï¼š{len(long_texts)} å€‹")

    if len(short_texts) > len(documents) * 0.3:
        print(f"   âš ï¸  è­¦å‘Šï¼š{len(short_texts)/len(documents)*100:.1f}% çš„æ–‡æœ¬éçŸ­")
    else:
        print(f"   âœ… æ–‡æœ¬é•·åº¦å“è³ªè‰¯å¥½")

    # æ¸¬è©¦ 3ï¼šä¸­è‹±æ–‡å…§å®¹æª¢æŸ¥
    print(f"\nğŸŒ æ¸¬è©¦ 3ï¼šä¸­è‹±æ–‡å…§å®¹æª¢æŸ¥")

    has_chinese = 0
    has_english = 0
    has_both = 0

    for doc in documents:
        text = doc.get('text', '')
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        english_chars = sum(1 for c in text if c.isalpha() and ord(c) < 128)

        if chinese_chars > 0:
            has_chinese += 1
        if english_chars > 20:  # è‡³å°‘ 20 å€‹è‹±æ–‡å­—æ¯
            has_english += 1
        if chinese_chars > 0 and english_chars > 20:
            has_both += 1

    print(f"   åŒ…å«ä¸­æ–‡ï¼š{has_chinese}/{len(documents)} ({has_chinese/len(documents)*100:.1f}%)")
    print(f"   åŒ…å«è‹±æ–‡ï¼š{has_english}/{len(documents)} ({has_english/len(documents)*100:.1f}%)")
    print(f"   ä¸­è‹±æ··åˆï¼š{has_both}/{len(documents)} ({has_both/len(documents)*100:.1f}%)")

    if has_both > len(documents) * 0.8:
        print(f"   âœ… å¤§éƒ¨åˆ†æ–‡ä»¶éƒ½æœ‰ä¸­è‹±æ–‡å…§å®¹ï¼Œé©åˆè·¨èªè¨€æª¢ç´¢")
    else:
        print(f"   âš ï¸  éƒ¨åˆ†æ–‡ä»¶ç¼ºå°‘ä¸­æ–‡æˆ–è‹±æ–‡å…§å®¹")

    # æ¸¬è©¦ 4ï¼šå…ƒè³‡æ–™å®Œæ•´æ€§
    print(f"\nğŸ“ æ¸¬è©¦ 4ï¼šå…ƒè³‡æ–™å®Œæ•´æ€§")

    has_name_zh = sum(1 for doc in documents if doc.get('metadata', {}).get('name_zh'))
    has_name_latin = sum(1 for doc in documents if doc.get('metadata', {}).get('name_latin'))
    has_url = sum(1 for doc in documents if doc.get('metadata', {}).get('url'))

    print(f"   æœ‰ä¸­æ–‡åï¼š{has_name_zh}/{len(documents)} ({has_name_zh/len(documents)*100:.1f}%)")
    print(f"   æœ‰å­¸åï¼š{has_name_latin}/{len(documents)} ({has_name_latin/len(documents)*100:.1f}%)")
    print(f"   æœ‰ URLï¼š{has_url}/{len(documents)} ({has_url/len(documents)*100:.1f}%)")

    if has_name_zh > len(documents) * 0.9 and has_name_latin > len(documents) * 0.9:
        print(f"   âœ… å…ƒè³‡æ–™å®Œæ•´æ€§è‰¯å¥½")
    else:
        print(f"   âš ï¸  éƒ¨åˆ†å…ƒè³‡æ–™ä¸å®Œæ•´")

    # æ¸¬è©¦ 5ï¼šç¯„ä¾‹å±•ç¤º
    print(f"\nğŸ“„ æ¸¬è©¦ 5ï¼šéš¨æ©Ÿç¯„ä¾‹å±•ç¤º")

    import random
    sample_doc = random.choice(documents)

    print(f"\n   æ–‡ä»¶ IDï¼š{sample_doc['id']}")
    print(f"   æ¤ç‰©åç¨±ï¼š{sample_doc['metadata'].get('name_zh', 'N/A')} ({sample_doc['metadata'].get('name_latin', 'N/A')})")
    print(f"   å®Œæ•´åº¦è©•åˆ†ï¼š{sample_doc['metadata'].get('completeness_score', 'N/A')}/5.0")
    print(f"   æ–‡æœ¬é•·åº¦ï¼š{len(sample_doc['text'])} å­—å…ƒ")
    print(f"\n   æ–‡æœ¬é è¦½ï¼š")
    print(f"   {'-'*66}")
    preview = sample_doc['text'][:200] + "..." if len(sample_doc['text']) > 200 else sample_doc['text']
    print(f"   {preview}")
    print(f"   {'-'*66}")

    # ç¸½çµ
    print(f"\n" + "=" * 70)
    print("æ¸¬è©¦ç¸½çµ")
    print("=" * 70)

    score = 0
    max_score = 5

    if not issues:
        score += 1
    if len(short_texts) < len(documents) * 0.3:
        score += 1
    if has_both > len(documents) * 0.8:
        score += 1
    if has_name_zh > len(documents) * 0.9:
        score += 1
    if has_name_latin > len(documents) * 0.9:
        score += 1

    print(f"\nâœ… å“è³ªè©•åˆ†ï¼š{score}/{max_score}")

    if score >= 4:
        print("ğŸ‰ RAG è³‡æ–™å“è³ªå„ªç§€ï¼Œå¯ä»¥ä½¿ç”¨ï¼")
        print("\nå»ºè­°çš„ä½¿ç”¨æ–¹å¼ï¼š")
        print("1. ä½¿ç”¨ jinaai/jina-embeddings-v2-base-zh å»ºç«‹å‘é‡ç´¢å¼•")
        print("2. æ”¯æ´ä¸­æ–‡æŸ¥è©¢è‡ªå‹•åŒ¹é…è‹±æ–‡æè¿°ï¼ˆè·¨èªè¨€æª¢ç´¢ï¼‰")
        print("3. å¯ç›´æ¥éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ")
    elif score >= 3:
        print("âš ï¸  RAG è³‡æ–™å“è³ªå°šå¯ï¼Œå»ºè­°æ”¹é€²å¾Œä½¿ç”¨")
    else:
        print("âŒ RAG è³‡æ–™å“è³ªä¸è¶³ï¼Œéœ€è¦æ”¹é€²")

    print("\n" + "=" * 70)


if __name__ == '__main__':
    test_rag_documents()
