# 🛡️ 爬虫防止中断完整指南

## 问题诊断：为什么会中途停止？

### ❌ 主要原因

1. **电脑睡眠/休眠（最常见！）**
   - 屏幕保护程序本身不会影响后台程序
   - 但电脑进入**睡眠模式**会暂停所有进程
   - 网络连接会断开
   - 爬虫进程会卡住或超时失败

2. **网络不稳定**
   - 超时后重试次数有限
   - 多次失败后静默跳过

3. **内存/资源限制**
   - 长时间运行可能导致资源耗尽

## ✅ 解决方案

### 方案 1：使用增强版爬虫（推荐！）

我已经为您创建了改进版本：

**新增功能：**
- ✅ **断点续传** - 自动跳过已下载的文件
- ✅ **增强重试** - 5次重试，指数退避（1, 2, 4, 8, 16秒）
- ✅ **实时状态保存** - 每10个保存一次进度
- ✅ **详细日志** - 记录所有操作
- ✅ **优雅中断** - Ctrl+C 时保存状态
- ✅ **失败记录** - 保存失败的编码供重试

**使用方法：**

```bash
# Linux/macOS（会自动防止睡眠）
./start_robust.sh

# Windows
start_robust.bat

# 或直接运行
python3 robust_crawler.py 1.5
```

### 方案 2：防止电脑睡眠

#### Windows

**方法 A：图形界面（推荐）**
1. 打开 **控制面板**
2. 进入 **电源选项**
3. 点击当前计划旁的 **更改计划设置**
4. 将 **使计算机进入睡眠状态** 设为 **从不**
5. 保存更改

**方法 B：命令行（管理员权限）**
```powershell
# 在 PowerShell（管理员）中运行
powercfg -change -standby-timeout-ac 0    # 接电源时不睡眠
powercfg -change -standby-timeout-dc 0    # 用电池时不睡眠
```

**方法 C：临时保持唤醒**
- 下载并使用 [Caffeine](https://www.zhornsoftware.co.uk/caffeine/) 工具
- 或使用 [Don't Sleep](https://www.softwareok.com/?seite=Microsoft/DontSleep) 工具

#### macOS

**方法 A：图形界面**
1. 打开 **系统偏好设置**
2. 进入 **节能**
3. 将 **电脑进入睡眠前的时间** 滑块调到 **永不**
4. 取消勾选 **在电池供电时，显示器关闭后将硬盘置入睡眠状态**

**方法 B：使用 caffeinate（自动）**
```bash
# start_robust.sh 已包含此命令
caffeinate -i python3 robust_crawler.py 1.5
```

#### Linux

**方法 A：GNOME 桌面**
1. 打开 **设置**
2. 进入 **电源**
3. 将 **自动待机** 设为 **从不**

**方法 B：使用 systemd-inhibit（自动）**
```bash
# start_robust.sh 已包含此命令
systemd-inhibit --what=idle:sleep --who="Plant Crawler" \
    --why="正在爬取植物数据" python3 robust_crawler.py 1.5
```

**方法 C：临时禁用**
```bash
# Ubuntu/Debian
gsettings set org.gnome.settings-daemon.plugins.power sleep-inactive-ac-timeout 0

# 恢复（设为 30 分钟）
gsettings set org.gnome.settings-daemon.plugins.power sleep-inactive-ac-timeout 1800
```

### 方案 3：后台运行（适合长时间爬取）

#### Linux/macOS

```bash
# 使用 nohup 后台运行
nohup python3 robust_crawler.py 1.5 > crawler.out 2>&1 &

# 查看进程
ps aux | grep robust_crawler

# 查看实时日志
tail -f plant_data/crawler.log

# 查看进度
python3 check_progress.py
```

#### Windows

```powershell
# 使用 PowerShell 后台运行
Start-Process python -ArgumentList "robust_crawler.py 1.5" -WindowStyle Hidden

# 或使用任务计划程序创建持久任务
```

### 方案 4：使用服务器/云端（永不睡眠）

**如果本机还是不稳定，可以考虑：**

1. **云服务器** - AWS EC2 / Google Cloud / 阿里云
2. **Raspberry Pi** - 低功耗 24/7 运行
3. **Docker 容器** - 在服务器上持续运行

## 📊 监控进度

### 实时查看进度

```bash
# 查看详细进度
python3 check_progress.py

# 查看日志最后几行
tail -20 plant_data/crawler.log

# 实时监控日志
tail -f plant_data/crawler.log
```

### 查看状态文件

```bash
# 查看 JSON 格式的状态
cat plant_data/crawler_status.json
```

## 🔄 中断后恢复

**如果爬虫中断了，不用担心！**

```bash
# 直接重新运行，会自动跳过已完成的
./start_robust.sh

# 或
python3 robust_crawler.py 1.5
```

**增强版爬虫会自动：**
1. 扫描已下载的文件
2. 跳过已完成的编码
3. 继续未完成的部分
4. 不会重复下载

## 🧪 测试建议

**第一次运行前，建议先测试：**

```bash
# 1. 创建测试文件（只包含5个编码）
head -5 plant_codes.txt > test_codes.txt

# 2. 修改 robust_crawler.py 中的 codes_file
# codes_file='test_codes.txt'

# 3. 运行测试
python3 robust_crawler.py 0.5

# 4. 检查是否正常工作
python3 check_progress.py

# 5. 如果成功，改回正式配置
# codes_file='plant_codes.txt'
```

## ⚡ 优化建议

### 调整延迟时间

```bash
# 更快（但要小心）
python3 robust_crawler.py 1.0

# 更慢但更安全
python3 robust_crawler.py 2.0

# 默认（推荐）
python3 robust_crawler.py 1.5
```

### 批量处理失败的编码

```bash
# 如果有失败的编码记录在 failed_codes.txt
# 可以修改 robust_crawler.py 读取这个文件重试
```

## 📋 完整运行检查清单

在开始爬取前，确认：

- [ ] ✅ 已安装依赖：`pip install -r requirements.txt`
- [ ] ✅ 已设置电脑不要睡眠
- [ ] ✅ 已连接稳定网络（有线更好）
- [ ] ✅ 已插电源（笔记本电脑）
- [ ] ✅ 已关闭自动更新（避免重启）
- [ ] ✅ 磁盘空间充足（至少 100MB）
- [ ] ✅ 已测试少量编码确认能正常工作

## 🆘 常见问题

### Q: 如何确认爬虫还在运行？

```bash
# Linux/macOS
ps aux | grep robust_crawler

# 或查看进程文件
ls -lh plant_data/crawler_status.json

# 查看最新日志时间
tail -1 plant_data/crawler.log
```

### Q: 如何停止爬虫？

```bash
# 优雅停止（会保存状态）
按 Ctrl+C

# 强制停止
pkill -f robust_crawler

# Windows
taskkill /F /IM python.exe
```

### Q: 如何知道剩余多久完成？

```bash
python3 check_progress.py
# 会显示预估剩余时间
```

### Q: 爬虫是否会重复下载？

不会！增强版会自动检测已存在的文件并跳过。

## 📞 需要帮助？

如果遇到问题：

1. 查看日志：`cat plant_data/crawler.log`
2. 查看进度：`python3 check_progress.py`
3. 查看失败的编码：`cat plant_data/failed_codes.txt`

---

**数据来源：** 台灣植物資訊整合查詢系統, https://tai2.ntu.edu.tw

**使用规范：** 请注明数据来源，仅供学术和教育用途。
