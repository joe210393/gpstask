# 🌿 台湾植物全量爬取完整指南

## 目标

爬取**台湾植物资料库的全部植物数据**（约 5,400 种植物）

## 📋 两步流程

### 第一步：发现所有植物编码 ⏱️ 1-48小时

使用 `discover_all_codes.py` 来扫描并找出所有有效的植物编码

### 第二步：爬取所有植物数据 ⏱️ 2-24小时

使用 `robust_crawler.py` 和发现的编码列表来爬取所有植物

---

## 🚀 详细步骤

### 准备工作

1. **确认依赖已安装**
```bash
cd /home/user/gpstask/scripts/plant_crawler
pip install -r requirements.txt
```

2. **关闭电脑睡眠**（✅ 您已完成）
   - Windows: 控制面板 > 电源选项 > 睡眠设为"从不"
   - macOS: 系统偏好设置 > 节能 > 永不睡眠
   - Linux: 设置 > 电源 > 自动待机设为"从不"

3. **确保网络稳定**
   - 有线网络更佳
   - 确保网络不会中断

---

### 第一步：发现所有植物编码

#### 🎯 推荐：快速模式（适合大多数情况）

```bash
python3 discover_all_codes.py
# 选择: 1 (快速模式)
```

**特点：**
- ⏱️ 耗时：约 1-2 小时
- 📊 覆盖范围：主要编码区间
- 🎯 预计发现：1,000-3,000 种植物
- ✅ 足够满足大多数需求

**扫描范围：**
- 第1段：100-600（步长5）
- 第2段：001-050
- 第3段：01-10
- 第4段：0-1

#### ⚡ 标准模式（更全面）

```bash
python3 discover_all_codes.py
# 选择: 2 (标准模式)
```

**特点：**
- ⏱️ 耗时：约 4-8 小时
- 📊 覆盖范围：常见编码区间
- 🎯 预计发现：3,000-4,500 种植物
- ✅ 推荐用于完整采集

**扫描范围：**
- 第1段：100-1000（步长5）
- 第2段：001-100
- 第3段：01-20
- 第4段：0-2

#### 🔥 完整模式（最全面，耗时最长）

```bash
python3 discover_all_codes.py
# 选择: 3 (完整模式)
```

**特点：**
- ⏱️ 耗时：约 24-48 小时
- 📊 覆盖范围：所有可能编码
- 🎯 预计发现：接近全部 5,400 种
- ⚠️ 仅在需要100%完整时使用

**扫描范围：**
- 第1段：100-1000（步长1）
- 第2段：001-200
- 第3段：01-50
- 第4段：0-9

#### ⚙️ 自定义模式

```bash
python3 discover_all_codes.py
# 选择: 4 (自定义模式)
# 然后输入自定义范围
```

#### 📊 监控进度

编码发现过程中会显示：
- 当前进度百分比
- 已发现的有效编码数量
- 预计剩余时间

**输出文件：**
- `all_plant_codes.txt` - 所有发现的编码（文本格式）
- `all_plant_codes.json` - 包含元数据的JSON格式

---

### 第二步：爬取所有植物数据

编码发现完成后，使用发现的编码列表进行全量爬取：

#### 方法 A：使用启动脚本（推荐）

**Linux/macOS:**
```bash
# 修改 start_robust.sh 中的文件名
# 将 plant_codes.txt 改为 all_plant_codes.txt
./start_robust.sh
```

**Windows:**
```bash
# 修改 start_robust.bat 中的文件名
start_robust.bat
```

#### 方法 B：直接运行

```bash
# 使用发现的完整编码列表
python3 robust_crawler.py all_plant_codes.txt 1.5
```

**参数说明：**
- `all_plant_codes.txt` - 编码文件
- `1.5` - 请求延迟（秒）

#### 📊 监控爬取进度

在另一个终端窗口运行：

```bash
# 查看实时进度
python3 check_progress.py

# 查看日志
tail -f plant_data/crawler.log

# 查看最新日志（最后20行）
tail -20 plant_data/crawler.log
```

**进度文件：**
- `plant_data/crawler_status.json` - 当前状态和进度
- `plant_data/crawler.log` - 详细日志
- `plant_data/failed_codes.txt` - 失败的编码（供重试）

---

## ⏱️ 时间估算

### 编码发现阶段

| 模式 | 测试数量 | 延迟0.5秒 | 延迟1秒 |
|------|---------|----------|---------|
| 快速 | ~10,000 | 1.4小时 | 2.8小时 |
| 标准 | ~80,000 | 11小时 | 22小时 |
| 完整 | ~1,800,000 | 250小时 | 500小时 |

### 爬取阶段

假设发现了 N 个有效编码：

| 编码数量 | 延迟1.5秒 | 延迟2秒 |
|---------|----------|---------|
| 1,000 | 0.4小时 | 0.6小时 |
| 3,000 | 1.3小时 | 1.7小时 |
| 5,400 | 2.3小时 | 3.0小时 |

**总耗时估算（标准模式）：**
- 编码发现：11小时（延迟0.5秒）
- 数据爬取：2小时（假设4000个，延迟1.5秒）
- **总计：约 13 小时**

---

## 🔄 中断恢复

### 编码发现中断

如果编码发现过程中断：
1. 已发现的编码**不会丢失**（但需要手动保存）
2. 目前暂不支持断点续传
3. 建议使用较小的扫描范围多次运行

**解决方案：**
- 使用快速或标准模式（而非完整模式）
- 或者分段扫描，手动合并结果

### 爬取中断

如果爬取过程中断：
1. ✅ 已爬取的数据**已保存**
2. ✅ 重新运行会**自动跳过**已完成的
3. ✅ 从断点继续，无需重新开始

```bash
# 直接重新运行即可
python3 robust_crawler.py all_plant_codes.txt 1.5
```

---

## 📊 数据统计

爬取完成后，可以生成统计信息：

```bash
# 查看统计
python3 statistics.py all_plant_codes.txt
```

---

## 🎯 推荐流程（最佳实践）

### 方案 A：快速启动（推荐）

**适合：想要快速获得大量数据，可接受部分缺失**

1. 运行快速编码发现（1-2小时）
2. 立即开始爬取（1-2小时）
3. **总耗时：2-4小时**
4. 预期结果：1,000-3,000种植物

```bash
# 第1步：快速发现
python3 discover_all_codes.py
# 选择: 1

# 第2步：等待完成，然后爬取
python3 robust_crawler.py all_plant_codes.txt 1.5
```

### 方案 B：标准流程（推荐）

**适合：需要较完整的数据集**

1. 运行标准编码发现（4-8小时）
2. 完成后开始爬取（2-3小时）
3. **总耗时：6-11小时**
4. 预期结果：3,000-4,500种植物

```bash
# 第1步：标准发现（建议睡前启动）
python3 discover_all_codes.py
# 选择: 2

# 第2步：等待完成，然后爬取
python3 robust_crawler.py all_plant_codes.txt 1.5
```

### 方案 C：分阶段进行

**适合：时间受限，分多次完成**

**阶段1（晚上）：**
```bash
# 启动快速发现，睡觉前运行
python3 discover_all_codes.py  # 选择1
```

**阶段2（第二天早上）：**
```bash
# 查看发现了多少
wc -l all_plant_codes.txt

# 启动爬取，上班前运行
python3 robust_crawler.py all_plant_codes.txt 1.5
```

**阶段3（回家后）：**
```bash
# 检查进度
python3 check_progress.py

# 如果完成，准备RAG数据
python3 prepare_rag_data_enhanced.py
```

---

## 🎁 额外功能

### 准备 RAG 数据

爬取完成后，转换为 RAG 格式：

```bash
# 增强版（推荐，含识别特征）
python3 prepare_rag_data_enhanced.py

# 基础版
python3 prepare_rag_data.py
```

### 测试数据质量

```bash
# 测试增强版 RAG 数据
python3 test_enhanced_rag.py

# 测试基础版
python3 test_rag_quality.py
```

---

## 🆘 故障排查

### 问题：编码发现速度太慢

**解决：**
- 降低延迟（但不建议低于0.3秒）
- 使用更小的扫描范围
- 选择快速模式而非完整模式

### 问题：发现的编码太少

**解决：**
- 检查网络连接
- 查看是否有错误日志
- 尝试扩大扫描范围
- 使用标准或完整模式

### 问题：爬取时频繁失败

**解决：**
- 增加延迟时间（1.5 → 2.0秒）
- 检查网络稳定性
- 查看 `failed_codes.txt` 分析失败原因

### 问题：电脑还是自动睡眠

**解决：**
- 再次检查电源设置
- 使用防睡眠软件（Windows: Caffeine, macOS: caffeinate）
- 使用启动脚本（自动调用防睡眠功能）

---

## 📞 需要帮助？

1. 查看日志：`cat plant_data/crawler.log`
2. 查看进度：`python3 check_progress.py`
3. 查看已下载数量：`ls plant_data/*.json | wc -l`

---

## 🎉 完成后

当爬取完成，您将拥有：

1. **原始数据**
   - `plant_data/*.json` - 每个植物的详细数据
   - `plant_data/all_plants.json` - 所有植物合集

2. **RAG 数据**
   - `plant_data/rag_documents_enhanced.json` - 增强版 RAG 文档
   - `plant_data/rag_documents_enhanced.jsonl` - JSONL 格式

3. **统计信息**
   - 总植物数量
   - 数据完整性
   - 特征覆盖率

**数据来源请注明：** 台灣植物資訊整合查詢系統, https://tai2.ntu.edu.tw
