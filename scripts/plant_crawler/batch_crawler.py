#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡æ¤ç‰©çˆ¬èŸ²
å¾æ¤ç‰©ç·¨ç¢¼åˆ—è¡¨æ‰¹é‡çˆ¬å–æ‰€æœ‰æ¤ç‰©è³‡æ–™ï¼Œæº–å‚™ RAG è³‡æ–™
"""

import json
import time
import os
from pathlib import Path
from typing import List, Dict
from plant_parser import PlantPageParser


class BatchCrawler:
    """æ‰¹é‡çˆ¬èŸ²"""

    def __init__(self, output_dir='./plant_data'):
        self.parser = PlantPageParser()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # çµ±è¨ˆè³‡æ–™
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'usable': 0,
            'start_time': time.time()
        }

    def load_codes(self, codes_file: str) -> List[str]:
        """è¼‰å…¥æ¤ç‰©ç·¨ç¢¼åˆ—è¡¨"""
        codes = []

        if codes_file.endswith('.json'):
            with open(codes_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                codes = data.get('codes', [])
        else:
            # æ–‡æœ¬æª”ï¼Œæ¯è¡Œä¸€å€‹ code
            with open(codes_file, 'r', encoding='utf-8') as f:
                codes = [line.strip() for line in f if line.strip()]

        return codes

    def crawl_all(self, codes: List[str], resume=True, delay=0.5):
        """
        æ‰¹é‡çˆ¬å–æ‰€æœ‰æ¤ç‰©

        Args:
            codes: æ¤ç‰©ç·¨ç¢¼åˆ—è¡¨
            resume: æ˜¯å¦è·³éå·²ç¶“çˆ¬å–çš„ï¼ˆæ”¯æ´æ–·é»çºŒå‚³ï¼‰
            delay: æ¯æ¬¡è«‹æ±‚ä¹‹é–“çš„å»¶é²ï¼ˆç§’ï¼‰
        """
        self.stats['total'] = len(codes)

        print("=" * 70)
        print("æ‰¹é‡æ¤ç‰©çˆ¬èŸ²")
        print("=" * 70)
        print(f"ç¸½æ•¸ï¼š{len(codes)} å€‹æ¤ç‰©")
        print(f"è¼¸å‡ºç›®éŒ„ï¼š{self.output_dir}")
        print(f"æ–·é»çºŒå‚³ï¼š{'æ˜¯' if resume else 'å¦'}")
        print(f"è«‹æ±‚å»¶é²ï¼š{delay} ç§’")
        print("=" * 70)

        all_plants = []

        for i, code in enumerate(codes, 1):
            print(f"\n[{i}/{len(codes)}] ", end='')

            # æª¢æŸ¥æ˜¯å¦å·²ç¶“çˆ¬å–é
            output_file = self.output_dir / f"{code.replace('+', '_')}.json"
            if resume and output_file.exists():
                print(f"â­ï¸  è·³éï¼ˆå·²å­˜åœ¨ï¼‰ï¼š{code}")
                self.stats['skipped'] += 1

                # è¼‰å…¥å·²æœ‰çš„è³‡æ–™
                try:
                    with open(output_file, 'r', encoding='utf-8') as f:
                        plant_data = json.load(f)
                        all_plants.append(plant_data)
                        if plant_data.get('completeness', {}).get('is_usable'):
                            self.stats['usable'] += 1
                except:
                    pass

                continue

            # çˆ¬å–æ¤ç‰©è³‡æ–™
            plant_data = self.parser.parse_plant(code)

            if plant_data:
                self.stats['success'] += 1

                # å„²å­˜å–®å€‹æ¤ç‰©è³‡æ–™
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(plant_data, f, ensure_ascii=False, indent=2)

                all_plants.append(plant_data)

                # çµ±è¨ˆå¯ç”¨è³‡æ–™
                if plant_data.get('completeness', {}).get('is_usable'):
                    self.stats['usable'] += 1
            else:
                self.stats['failed'] += 1
                print(f"  âš ï¸  çˆ¬å–å¤±æ•—")

            # ç¦®è²Œæ€§å»¶é²
            time.sleep(delay)

            # æ¯ 10 å€‹æ¤ç‰©é¡¯ç¤ºä¸€æ¬¡é€²åº¦
            if i % 10 == 0:
                self._print_progress()

        # å„²å­˜åˆä½µçš„è³‡æ–™é›†
        self._save_combined_dataset(all_plants)

        # é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ
        self._print_final_stats()

        return all_plants

    def _print_progress(self):
        """é¡¯ç¤ºé€²åº¦"""
        completed = self.stats['success'] + self.stats['failed'] + self.stats['skipped']
        progress = completed / self.stats['total'] * 100 if self.stats['total'] > 0 else 0
        elapsed = time.time() - self.stats['start_time']

        print(f"\n  ğŸ“Š é€²åº¦ï¼š{completed}/{self.stats['total']} ({progress:.1f}%)")
        print(f"     æˆåŠŸï¼š{self.stats['success']} | å¤±æ•—ï¼š{self.stats['failed']} | è·³éï¼š{self.stats['skipped']}")
        print(f"     å¯ç”¨è³‡æ–™ï¼š{self.stats['usable']}")
        print(f"     å·²ç”¨æ™‚ï¼š{elapsed:.1f} ç§’")

    def _print_final_stats(self):
        """é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ"""
        elapsed = time.time() - self.stats['start_time']

        print("\n" + "=" * 70)
        print("çˆ¬å–å®Œæˆï¼")
        print("=" * 70)
        print(f"ç¸½æ•¸ï¼š{self.stats['total']}")
        print(f"æˆåŠŸï¼š{self.stats['success']}")
        print(f"å¤±æ•—ï¼š{self.stats['failed']}")
        print(f"è·³éï¼š{self.stats['skipped']}")
        print(f"å¯ç”¨è³‡æ–™ï¼š{self.stats['usable']} ({self.stats['usable']/self.stats['total']*100:.1f}%)")
        print(f"ç¸½ç”¨æ™‚ï¼š{elapsed:.1f} ç§’")
        print(f"å¹³å‡é€Ÿåº¦ï¼š{self.stats['total']/elapsed:.2f} å€‹/ç§’" if elapsed > 0 else "")
        print("=" * 70)

    def _save_combined_dataset(self, plants: List[Dict]):
        """å„²å­˜åˆä½µçš„è³‡æ–™é›†"""
        # å®Œæ•´è³‡æ–™é›†
        full_dataset = {
            'metadata': {
                'total': len(plants),
                'usable': sum(1 for p in plants if p.get('completeness', {}).get('is_usable')),
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'source': 'https://tai2.ntu.edu.tw',
                'description': 'å°ç£æ¤ç‰©è³‡è¨Šæ•´åˆæŸ¥è©¢ç³»çµ±'
            },
            'plants': plants
        }

        output_file = self.output_dir / 'all_plants.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(full_dataset, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… å®Œæ•´è³‡æ–™é›†å·²å„²å­˜ï¼š{output_file}")

        # åªä¿å­˜å¯ç”¨çš„æ¤ç‰©ï¼ˆç”¨æ–¼ RAGï¼‰
        usable_plants = [p for p in plants if p.get('completeness', {}).get('is_usable')]

        rag_dataset = {
            'metadata': {
                'total': len(usable_plants),
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'source': 'https://tai2.ntu.edu.tw',
                'description': 'å°ç£æ¤ç‰©è³‡è¨Š - RAG å¯ç”¨è³‡æ–™é›†',
                'filter': 'is_usable=True (æœ‰ä¸­æ–‡åæˆ–å­¸å + æœ‰ç‰¹å¾µæè¿°)'
            },
            'plants': usable_plants
        }

        rag_file = self.output_dir / 'rag_plants.json'
        with open(rag_file, 'w', encoding='utf-8') as f:
            json.dump(rag_dataset, f, ensure_ascii=False, indent=2)

        print(f"âœ… RAG è³‡æ–™é›†å·²å„²å­˜ï¼š{rag_file} ({len(usable_plants)} å€‹å¯ç”¨æ¤ç‰©)")


def main():
    """ä¸»å‡½æ•¸"""
    import sys

    # é è¨­åƒæ•¸
    codes_file = 'plant_codes.txt'
    output_dir = './plant_data'
    delay = 0.5

    # å¾å‘½ä»¤åˆ—åƒæ•¸è®€å–
    if len(sys.argv) > 1:
        codes_file = sys.argv[1]
    if len(sys.argv) > 2:
        output_dir = sys.argv[2]
    if len(sys.argv) > 3:
        delay = float(sys.argv[3])

    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(codes_file):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ¤ç‰©ç·¨ç¢¼æª”æ¡ˆ {codes_file}")
        print("\nç”¨æ³•ï¼š")
        print("  python batch_crawler.py [codes_file] [output_dir] [delay]")
        print("\nç¯„ä¾‹ï¼š")
        print("  python batch_crawler.py plant_codes.txt ./plant_data 0.5")
        sys.exit(1)

    # å‰µå»ºçˆ¬èŸ²
    crawler = BatchCrawler(output_dir=output_dir)

    # è¼‰å…¥ç·¨ç¢¼
    codes = crawler.load_codes(codes_file)
    print(f"âœ… è¼‰å…¥äº† {len(codes)} å€‹æ¤ç‰©ç·¨ç¢¼")

    # é–‹å§‹çˆ¬å–
    crawler.crawl_all(codes, resume=True, delay=delay)


if __name__ == '__main__':
    main()
